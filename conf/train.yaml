# train.yaml

verification_model:
  load_saved_model: False #load a pre-trained model from ${verification_model.model_path} (True), or build and train a new model (False)
  model_path: ${callbacks.eer_monitor.model_path}
  resnet:
    _target_: classification_models.tfkeras.Classifiers.get
    _args_: ['resnet34']
  resnet_name: ResNet34_Base
  global_pool:
    _target_: tensorflow.keras.layers.GlobalAveragePooling2D
  model:
    _partial_: true
    _target_: src.models.VerificationModel
    normalization_layer:
      _target_: src.models.L2Normalization
    cosine_layer:
      _partial_: true
      _target_: src.models.CosineLayer
      use_bias: False
      name: cosine_softmax
    embedding_dim: 512
    return_embedding: False
    name: verification_model_resnet34_512dim

dataset:
  _target_: src.data.create_training_dataset
  dev1_csv: csv/vc2_dev_ok.csv
  dev2_csv: csv/vc2_test_ok.csv
  batch_size: 32
  eer_val: True # whether to use all data for training, validation on ${eer_testset}
  preprocess_audio: False

# Test set for EER evaluation
eer_testset: csv/VoxCeleb1-E.csv
  
tensorboard:
  log_dir: logs/fit/run1
  histogram_freq: 1

# main training phase: from scratch
stage1:
  execute: True
  initial_epoch: 0
  epochs: 10
  loss_fn_partial:
    _partial_: true
    _target_: src.models.AdaCosLoss
  optimizer:
    _target_: tensorflow.keras.optimizers.SGD
    learning_rate: 0.01
    momentum: 0.9
    weight_decay: 5e-4
  metrics: sparse_categorical_accuracy

# fine-tuning phase
stage2:
  execute: True
  resume_from_best: True # whether to load best model from stage1 ${callbacks.eer_monitor.model_path}
  initial_epoch: ${stage1.epochs}
  epochs: 16
  optimizer:
    _target_: tensorflow.keras.optimizers.SGD
    learning_rate: 0.0005
    momentum: 0.9
    weight_decay: 1e-1
  metrics: sparse_categorical_accuracy
  eer_factor: 0.2
  eer_min_lr: ${callbacks.reduce_lr_on_plateau.min_lr}

callbacks:
  eer_monitor:
    _partial_: true
    _target_: src.models.EERMonitor
    model_path: models/best_model.keras
    eer_fn:
      _partial_: true
      _target_: src.eval.eer
      batch_size: ${dataset.batch_size}
    log_dir: ${tensorboard.log_dir}
    patience: 5 # changed in stage2 to 2 (for quicker lr changes)
    steps_interval: [10000, 20000, 28000]

  reduce_lr_on_plateau:
    _target_: tensorflow.keras.callbacks.ReduceLROnPlateau
    monitor: loss
    factor: 0.3
    patience: 0
    min_delta: 0.25
    verbose: 1
    mode: min
    min_lr: 1e-9

  lr_scheduler_partial:
    _partial_: true
    _target_: tensorflow.keras.callbacks.LearningRateScheduler

  checkpoint:
    _target_: tensorflow.keras.callbacks.ModelCheckpoint
    filepath: checkpoint/ckpResNet34_{epoch:02d}_valLoss{loss:.4f}_valAcc{sparse_categorical_accuracy:.4f}.keras
    monitor: sparse_categorical_accuracy
    save_best_only: False
    save_weights_only: False
    verbose: 1

  tensorboard_callback:
    _target_: tensorflow.keras.callbacks.TensorBoard
    log_dir: ${tensorboard.log_dir}
    histogram_freq: ${tensorboard.histogram_freq}
